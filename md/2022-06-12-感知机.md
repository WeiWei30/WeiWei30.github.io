## 单层感知机模型

<img src="../img/2022-06-12-感知机/image-20220612164317987.png" alt="image-20220612164317987" style="zoom:50%;" />

单层感知机只有一个输入层和一个输出层。其输出为：
$$
y=f(\pmb{w}^T\pmb{x}-\theta)
$$
其中$\pmb{w},\pmb{x}\in\mathbb{R}^d$。$f(\cdot)$常为符号函数$\text{sign}(\cdot)$，即：
$$
y=\left\{
\begin{split}
1,\ &\pmb{w}^T\pmb{x}-\theta\ge0\\
0,\ &\pmb{w}^T\pmb{x}-\theta<0
\end{split}
\right.
$$
从几何的角度，感知机可以理解为：**用一个$d$维超平面去分割空间，位于超平面之上的为1，反之为0。**

因此感知机的损失函数可以定义为：**所有误分类点到超平面距离之和**。

---

点到平面的距离为：
$$
d(\pmb{x}_0)=\cfrac{|\pmb{w}^T\pmb{x}-\theta|}{||\pmb{w}||_2^2}
$$

---

误分类点越少、误分类点到超平面距离越近，损失函数就越小。有两种误分类情况：

1. 标签$y$为1，预测$\hat{y}$为0，此时$\pmb{w}^T\pmb{x}-\theta<0$
2. 标签$y$为0，预测$\hat{y}$为1。此时$\pmb{w}^T\pmb{x}-\theta\ge0$

假设有$m$个误分类样本，两种情况下误分类点到超平面距离之和可以统一为：
$$
L(\pmb{w},\theta)=\sum_{i=1}^m (\hat{y}-y)\cfrac{\pmb{w}^T\pmb{x}_i-\theta}{||\pmb{w}||_2^2}
$$
不考虑$||\pmb{w}||_2^2$，可写成：
$$
L(\pmb{w},\theta)=\sum_{i=1}^m (\hat{y}-y)(\pmb{w}^T\pmb{x}_i-\theta)
$$
令$\theta$为一个哑神经元节点的权重，即该神经元的输入恒为$-1$，权重为$\theta$。令$\hat{\pmb{w}}=[\pmb{w};\theta],\hat{\pmb{x}}=[\pmb{x};-1]$，上式可简化为：
$$
L(\hat{\pmb{w}})=\sum_{i=1}^m (\hat{y}-y)\hat{\pmb{w}}^T\hat{\pmb{x}_i}
$$
对$\hat{\pmb{w}}$求导可得：
$$
\cfrac{\part L}{\part \hat{\pmb{w}}}=\sum_{i=1}^m(\hat{y}-y)\hat{\pmb{x}}_i
$$
==感知机采用随机梯度下降(SGD)==，即随机选取一个样本的负梯度方向作为下降方向。则权重的更新为：
$$
\pmb{w}\gets\pmb{w}-\eta(\hat{y}-y)\hat{\pmb{x}_i}
$$

---

**PS：**

1. 随机梯度下降：损失函数为标准BP，即以一个样本损失函数的负梯度方向作为下降方向。
2. 梯度下降：损失函数为累积BP，即以所有样本损失函数之和作为新的损失函数，以该损失函数的负梯度方向作为下降方向。
3. mini-batch随机梯度下降：损失函数为累积BP，以一个小批次的损失函数之和作为新的损失函数，以该损失函数的负梯度方向作为下降方向。神经网络的训练通常采用此种方式。