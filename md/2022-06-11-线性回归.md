## 问题

给定包含$m$个数据对的数据集$D=\{(\pmb{x}_i,y_i)\}_{i=1}^m$，其中$\pmb{x}_i=[x_{i1},x_{i2},\cdots,x_{id}]^T$，每个样本包含$d$个属性。线性回归即利用一个线性模型去预测输出。

## 一个属性(最小二乘法)

首先分析输入只有一个属性即$d=1$的情况，此时$D=\{(x_i,y_i)\}_{i=1}^m$。线性回归即求使得预测点$f(x_i)$到直线的欧氏距离之和最小的权重$w^*$和偏执$b^*$：
$$
f(x_i)=wx_i+b
$$

$$
\begin{split}
(w^*,b^*)&=\mathop{\text{arg min}}_\limits{(w,b)}\sum_{i=1}^m(f(x_i)-y_i)^2\\
&=\mathop{\text{arg min}}_\limits{(w,b)}\sum_{i=1}^m(y_i-wx_i-b)^2
\end{split}
$$

令$E(w,b)=\sum_{i=1}^m(y_i-wx_i-b)^2$，其为关于$w$和$b$的凸函数[mark]，因此当偏导数为零时，可取得最值。
$$
\begin{split}
\cfrac{\part E(w,b)}{\part w}&=2\sum_{i=1}^m-x_i(y_i-wx_i-b)\\
&=2\left(w\sum_{i=1}^mx_i^2-\sum_{i=1}^m(y_i-b)x_i\right)
\end{split}\label{2}
$$

$$
\begin{split}
\cfrac{\part E(w,b)}{\part b}&=2\sum_{i=1}^m-(y_i-wx_i-b)\\
&=2\left(mb-\sum_{i=1}^m(y-wx_i)\right)
\end{split}\label{3}
$$

令$\eqref{3}$式等于0可得：
$$
\begin{split}
b&=\cfrac{1}{m}\sum_{i=1}^m(y_i-wx_i)\\
&=\bar{y}-w\bar{x}
\end{split}\label{4}
$$
令$\eqref{2}$式等于0，并将$\eqref{4}$式代入可得：
$$
\begin{split}
w\sum_{i=1}^mx_i^2&=\sum_{i=1}^m(y_i-\bar{y}+w\bar{x})x_i\\
w\left(\sum_{i=1}^mx_i^2-\bar{x}\sum_{i=1}^mx_i\right)&=\sum_{i=1}^m(y_i-\bar{y})x_i\\
w&=\cfrac{\sum_{i=1}^mx_iy_i-\sum_{i=1}^m\bar{y}x_i}{\sum_{i=1}^mx_i^2-\bar{x}\sum_{i=1}^mx_i}\\
&=\cfrac{\sum_{i=1}^mx_iy_i-\sum_{i=1}^m\bar{y}x_i-\sum_{i=1}^m\bar{y}x_i+\sum_{i=1}^m\bar{y}x_i}{\sum_{i=1}^mx_i^2-\bar{x}\sum_{i=1}^mx_i-\bar{x}\sum_{i=1}^mx_i+\bar{x}\sum_{i=1}^mx_i}\\
&=\cfrac{\sum_{i=1}^mx_iy_i-\sum_{i=1}^m\bar{y}x_i-\sum_{i=1}^m\bar{x}y_i+\sum_{i=1}^m\bar{x}\bar{y}}{\sum_{i=1}^mx_i^2-\sum_{i=1}^m2\bar{x}x_i+\sum_{i=1}^m\bar{x}^2}\\
&=\cfrac{\sum_{i=1}^m(x_i-\bar{x})(y_i-\bar{y})}{\sum_{i=1}^m(x_i-\bar{x})^2}
\end{split}
$$
令$\pmb{x}=[x_1,x_2,\cdots,x_m]^T$，$\pmb{y}=[y_1,y_2,\cdots,y_m]^T$，$\pmb{x}_d=\pmb{x}-\bar{x}\pmb{I}$，$\pmb{y}_d=\pmb{y}-\bar{y}\pmb{I}$，则：
$$
w=\cfrac{\pmb{x}_d^T\pmb{y}_d}{\pmb{x}_d^T\pmb{x}_d}
$$

## 多个属性

此时：
$$
f(\pmb{x}_i)=\pmb{w}^T\pmb{x}+b
$$
为求得欧氏距离之和最小：
$$
(\pmb{w}^*,b^*)=\mathop{\text{arg min}}_\limits{(w,b)}\sum_{i=1}^m(y_i-\pmb{w}^T\pmb{x}-b)\label{9}
$$
令$\hat{w}=[\pmb{w};b]^T$，$\pmb{y}=[y_1,y_2,\cdots,y_m]^T$，
$$
\textbf{X}=
\begin{bmatrix}
x_{11} & x_{12} & \cdots & x_{1d} & 1\\
x_{21} & x_{22} & \cdots & x_{2d} & 1\\
\vdots & \vdots & \ddots & \vdots & \vdots\\
x_{m1} & x_{m2} & \cdots & x_{md} & 1\\
\end{bmatrix}
$$
可将$\eqref{9}$式写成：
$$
(\hat{\pmb{w}}^*)=\mathop{\text{arg min}}_\limits{(\hat{w})}(\pmb{y}-\textbf{X}\hat{\pmb{w}})^T(\pmb{y}-\textbf{X}\hat{\pmb{w}})\label{11}
$$

---

$$
\cfrac{\part \pmb{a}^T\pmb{x}}{\part\pmb{x}}=\cfrac{\part \pmb{x}^T\pmb{a}}{\part\pmb{x}}=\pmb{a}
$$

$$
\cfrac{\part\pmb{x}^T\textbf{A}\pmb{x}}{\part \pmb{x}}=(\textbf{A}+\textbf{A}^T)\pmb{x}
$$

---

为求得最小值，令$E(\hat{\pmb{w}})=(\pmb{y}-\textbf{X}\hat{\pmb{w}})^T(\pmb{y}-\textbf{X}\hat{\pmb{w}})=\pmb{y}^T\pmb{y}-\hat{\pmb{w}}^T\textbf{X}^T\pmb{y}-\pmb{y}^T\textbf{X}\hat{\pmb{w}}+\hat{\pmb{w}}^T\textbf{X}^T\textbf{X}\hat{\pmb{w}}$，对其求偏导：
$$
\begin{split}
\cfrac{\part E(\hat{\pmb{w}})}{\part\hat{\pmb{w}}}&=\cfrac{\part\pmb{y}^T\pmb{y}}{\part\hat{\pmb{w}}}-\cfrac{\part\hat{\pmb{w}}^T\textbf{X}^T\pmb{y}}{\part\hat{\pmb{w}}}-\cfrac{\part\pmb{y}^T\textbf{X}\hat{\pmb{w}}}{\part\hat{\pmb{w}}}+\cfrac{\part\hat{\pmb{w}}^T\textbf{X}^T\textbf{X}\hat{\pmb{w}}}{\part\hat{\pmb{w}}}\\
&=-\textbf{X}^T\pmb{y}-\textbf{X}^T\pmb{y}+(\textbf{X}^T\textbf{X}+\textbf{X}^T\textbf{X})\hat{\pmb{w}}\\
&=2(\textbf{X}^T\textbf{X}\hat{\pmb{w}}-\textbf{X}^T\pmb{y})
\end{split}
$$

### $\textbf{X}^T\textbf{X}$满秩

当$\textbf{X}^T\textbf{X}$满秩时，可得
$$
\hat{\pmb{w}}=(\textbf{X}^T\textbf{X})^{-1}\textbf{X}^T\pmb{y}
$$
当$\textbf{X}^T\textbf{X}$不是满秩矩阵时

### $\textbf{X}^T\textbf{X}$非满秩

#### 小批量随机梯度下降

#### 

